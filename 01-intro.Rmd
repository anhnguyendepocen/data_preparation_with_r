
# Introduction {#intro}

Data is proliferating, collected as we go about our lives. Data about your day today might be collected when you 

* bought a cup of coffee (which generates at least three types of data: about the type of drink you ordered, about your coffee habits through the rewards program, and about the financial transaction—which is used by both the store and by you); 

* travelled to your place of work or study, captured through the movement of your cell phone; 

* renewed your driver's license (recording information about you such as date of birth, height, weight, and current address);

* went to the gym, and your pulse and breathing rates were captured through your wearable tech; and

* watched a movie on a streaming service, who build a profile about your viewing habits, and make recommendations as to what you might want to watch next.

***

A lot has been written about the benefits of organizations being "data-driven", when good data analysis leads to better decisions. There is no limit to the contexts where data analysis is being applied. In the private sector businesses can compete on analytics [@Davenport_Harris_2007], including retailers [@stitchfix_algo] and supply chain management [@Ashton_2018]. It can also be used in the public sector: data analysis can inform 
public health strategies during a pandemic [@Polonsky_etal_2019].

The process of going from raw data to decision support is often called data science. It combines the academic disciplines of statistics, computer programming, and the subject matter (be it astronomy, psychology, economics, or business). [@Conway_Venn_2010]

The typical data science process has been described in this model [@Wickham_Grolemund2016], and The generalist data scientist has a hand in all the steps through this cycle [@Monkman_2019]:

![the data science process](data-science.png)




Most data science and statistics text books use example data sets that are nicely formatted and easy to import.  Anscombe's Quartet [@Anscombe_1973] is a famous example of a small data set that was designed to produce specific statistical results. Other data sets might be chosen for their statistical properties, or compiled as examples to demonstrate a technique [@R-palmerpenguins], [@R-modeldata]. 

But in real life the data we find ourselves working with is, more often than not, entirely different from those text book examples. 

* The data we need might not exist yet, so we have to design our own data collection process.

* Data can be stored in different formats–spreadsheets, databases, software-specific file formats, websites, and PDF files. We need to have tools to access the data, no matter the file format.

* The data might be stored in an untidy layout. [@tidydata]

* We might receive data from multiple sources, so we end up with two files that have the same value but coded in different ways. For example, my home province in Canada can be represented as "British Columbia", "B.C.", or "BC". All three are accurate, but can cause problems if they are inconsistent across the data sources. And don't get me started on date formats—we'll save that for later.

* Data might have as-of-yet undetected errors. It might be incomplete, inconsistent, or it might have data entry mistakes and typos.

* It might have bias...and there are many different types of bias.

* It might contain sensitive information, and our sharing of the data, including the publication of reports, needs to protect the individual records of the people or businesses that provided the information. For example, you might be working with health records or tax records. Any disclosure of an individual record might be considered a privacy breach...so you have to take great care to ensure that what you've published doesn't violate any legal, ethical, or moral frameworks.


This book will introduce steps to ensure that you can assemble your data in a way that allows you to undertake the rest of the data science process, including making data and analysis available for a wider audience, including other data analysts who might seek to use the data in further analysis. 

Much of what is required in these steps can be programmed in the statistical programming language R, so that your efforts are reproducible. (For more about what is meant by "reproducibility", see [@Gandrud_2015] and [@Smith_reproducible_2017].) There are many packages that have been developed to explicitly tackle these challenges; this book isn't intended to catalogue them all, but to provide a current survey of ones commonly used. The sections that pertain to connecting to, querying, and manipulating databases will delve into the database query language SQL.



## Structure of this book

This book is divided into major sections, each with chapters that delve into strategies.

### 1. Acquiring data

Importing data from different file formats

[Webscraping]: pulling data from websites

[Connecting to databases]: connecting and querying


### 2. Transforming data

[Cleaning data]

* Missing values

* Inconsistency

* Finding duplicates

Data validation

### 3. Data transformation

Structure

Manipulation: creating new variables


### 4. Storing data



### 5. Sharing data

File formats

Data documentation

[Anonymity and Confidentiality]



## Jumping from the tutorials into your own data

https://twitter.com/beeonaposy/status/1281301404418703364?s=20






